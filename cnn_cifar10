import numpy as np
import pickle
from sklearn.metrics import accuracy_score


# 加载数据
def load_data():
    # 加载训练集数据
    x_train = []
    y_train = []
    for i in range(1, 6):
        with open(f'D:/人工智能导论/第二次实验/cifar_10/cifar-10-batches-py/data_batch_{i}', 'rb') as f:
            batch = pickle.load(f, encoding='bytes')
        x_train.append(batch[b'data'])
        y_train.append(batch[b'labels'])
    x_train = np.concatenate(x_train)
    y_train = np.concatenate(y_train)

    # 加载测试集数据
    with open(f'D:/人工智能导论/第二次实验/cifar_10/cifar-10-batches-py/test_batch', 'rb') as f:
        test_batch = pickle.load(f, encoding='bytes')
    x_test = test_batch[b'data']
    y_test = np.array(test_batch[b'labels'])

    return (x_train, y_train), (x_test, y_test)


# ReLU激活函数
def relu(x):
    return np.maximum(0, x)


# Softmax函数
def softmax(x):
    exp_scores = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)


# 计算损失函数
def compute_loss(y, y_pred):
    num_examples = y.shape[0]
    log_probs = -np.log(y_pred[range(num_examples), y])
    loss = np.sum(log_probs) / num_examples
    return loss



def conv2d(x, weights, b, stride=1, padding=0):
    # Padding
    if padding > 0:
        x = np.pad(x, ((0, 0), (padding, padding), (padding, padding), (0, 0)), mode='constant')

    # Convolution
    N, H, W, _ = x.shape
    HH, WW, _, num_filters = weights.shape
    H_out = 1 + (H + 2 * padding - HH) // stride
    W_out = 1 + (W + 2 * padding - WW) // stride
    out = np.zeros((N, H_out, W_out, num_filters))

    for i in range(H_out):
        for j in range(W_out):
            x_slice = x[:, i * stride:i * stride + HH, j * stride:j * stride + WW, :]
            print("x_slice shape:", x_slice.shape)
            print("weights shape:", weights.shape)

            out[:, i, j, :] = np.sum(x_slice * weights, axis=(1, 2,3)) + b

    return out



# 最大池化操作
def max_pool2d(x, pool_size=2, stride=2):
    N, H, W, C = x.shape
    H_out = (H - pool_size) // stride + 1
    W_out = (W - pool_size) // stride + 1
    out = np.zeros((N, H_out, W_out, C))

    for i in range(H_out):
        for j in range(W_out):
            x_slice = x[:, i * stride:i * stride + pool_size, j * stride:j * stride + pool_size, :]
            out[:, i, j, :] = np.max(x_slice, axis=(1, 2))

    return out


# 数据加载
(x_train, y_train), (x_test, y_test) = load_data()

# 数据预处理
x_train = x_train.reshape(x_train.shape[0], 3, 32, 32) / 255.0
x_test = x_test.reshape(x_test.shape[0], 3, 32, 32) / 255.0

num_classes = 10
num_samples = x_train.shape[0]

# 网络参数
hidden_size = 512
learning_rate = 0.001
num_epochs = 50
batch_size = 128

# 权重初始化
# 权重初始化
np.random.seed(0)
W1 = np.random.randn(3, 3, 3, 32) * 0.01
b1 = np.zeros((1, 32))
W2 = np.random.randn(3, 3, 32, 64) * 0.01
b2 = np.zeros((1, 64))
W3 = np.random.randn(64 * 8 * 8, hidden_size) * 0.01
b3 = np.zeros((1, hidden_size))
W4 = np.random.randn(hidden_size, num_classes) * 0.01
b4 = np.zeros((1, num_classes))

# 训练网络
# 训练网络
train_losses = []
test_losses = []
train_accuracies = []
test_accuracies = []

for epoch in range(num_epochs):
    # 打乱训练集顺序
    indices = np.arange(num_samples)
    np.random.shuffle(indices)
    x_train_shuffled = x_train[indices]
    y_train_shuffled = y_train[indices]

    # 分批训练
    for i in range(0, num_samples, batch_size):
        x_batch = x_train_shuffled[i:i + batch_size]
        y_batch = y_train_shuffled[i:i + batch_size]
        # 前向传播
        z1 = conv2d(x_batch, W1, b1)  # Convolution
        a1 = relu(z1)  # ReLU activation
        a1_pool = max_pool2d(a1)  # Max pooling
        z2 = conv2d(a1_pool, W2, b2)  # Convolution
        a2 = relu(z2)  # ReLU activation
        a2_pool = max_pool2d(a2)  # Max pooling
        flattened = a2_pool.reshape(a2_pool.shape[0], -1)  # Flatten
        z3 = np.dot(flattened, W3) + b3  # Fully connected layer
        a3 = relu(z3)  # ReLU activation
        z4 = np.dot(a3, W4) + b4  # Fully connected layer
        a4 = softmax(z4)  # Softmax activation

        # 计算损失
        loss = compute_loss(y_batch, a4)

        # 反向传播
        delta_z4 = a4.copy()
        batch_indices = np.arange(y_batch.shape[0])  # 使用实际批次大小的索引
        delta_z4[batch_indices, y_batch] -= 1
        delta_z4 /= y_batch.shape[0]

        dW4 = np.dot(a3.T, delta_z4)
        db4 = np.sum(delta_z4, axis=0, keepdims=True)

        delta_a3 = np.dot(delta_z4, W4.T)
        delta_z3 = delta_a3 * (z3 > 0)

        dW3 = np.dot(flattened.T, delta_z3)
        db3 = np.sum(delta_z3, axis=0, keepdims=True)

        delta_a2_pool = np.dot(delta_z3, W3.T)
        delta_a2_pool = delta_a2_pool.reshape(a2_pool.shape)  # Reshape
        delta_z2 = delta_a2_pool * (z2 > 0)

        dW2 = conv2d(a1_pool.transpose(0, 3, 1, 2), delta_z2, np.zeros_like(b2),
                     padding=1).transpose(1, 2, 3, 0)
        db2 = np.sum(delta_z2, axis=(0, 1, 2), keepdims=True)

        delta_a1_pool = conv2d(delta_z2, W2.transpose(1, 0, 2, 3), np.zeros_like(b1), padding=1)
        delta_a1_pool = delta_a1_pool.transpose(0, 2, 3, 1)  # Reshape
        delta_z1 = delta_a1_pool * (z1 > 0)

        dW1 = conv2d(x_batch.transpose(0, 3, 1, 2), delta_z1, np.zeros_like(b1),
                     padding=1).transpose(1, 2, 3, 0)
        db1 = np.sum(delta_z1, axis=(0, 1, 2), keepdims=True)

        print("dW1:", dW1)
        print("db1:", db1)
        print("dW2:", dW2)
        print("db2:", db2)
        print("dW3:", dW3)
        print("db3:", db3)
        print("dW4:", dW4)
        print("db4:", db4)

        # 参数更新
        W1 -= learning_rate * dW1
        b1 -= learning_rate * db1
        W2 -= learning_rate * dW2
        b2 -= learning_rate * db2
        W3 -= learning_rate * dW3
        b3 -= learning_rate * db3
        W4 -= learning_rate * dW4
        b4 -= learning_rate * db4

    # 在每个epoch结束后计算训练集和测试集上的损失和精度
    # 训练集
    z1_train = conv2d(x_train, W1, b1)
    a1_train = relu(z1_train)
    a1_pool_train = max_pool2d(a1_train)
    z2_train = conv2d(a1_pool_train, W2, b2)
    a2_train = relu(z2_train)
    a2_pool_train = max_pool2d(a2_train)
    flattened_train = a2_pool_train.reshape(a2_pool_train.shape[0], -1)
    z3_train = np.dot(flattened_train, W3) + b3
    a3_train = relu(z3_train)
    z4_train = np.dot(a3_train, W4) + b4
    a4_train = softmax(z4_train)
    train_loss = compute_loss(y_train, a4_train)
    train_losses.append(train_loss)
    train_pred = np.argmax(a4_train, axis=1)
    train_accuracy = accuracy_score(y_train, train_pred)
    train_accuracies.append(train_accuracy)

    # 测试集
    z1_test = conv2d(x_test, W1, b1)
    a1_test = relu(z1_test)
    a1_pool_test = max_pool2d(a1_test)
    z2_test = conv2d(a1_pool_test, W2, b2)
    a2_test = relu(z2_test)
    a2_pool_test = max_pool2d(a2_test)
    flattened_test = a2_pool_test.reshape(a2_pool_test.shape[0], -1)
    z3_test = np.dot(flattened_test, W3) + b3
    a3_test = relu(z3_test)
    z4_test = np.dot(a3_test, W4) + b4
    a4_test = softmax(z4_test)
    test_loss = compute_loss(y_test, a4_test)
    test_losses.append(test_loss)
    test_pred = np.argmax(a4_test, axis=1)
    test_accuracy = accuracy_score(y_test, test_pred)
    test_accuracies.append(test_accuracy)

    print(f"Epoch {epoch + 1}/{num_epochs}:")
    print(f"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}")
    print(f"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}")
    print()

import matplotlib.pyplot as plt
# 绘制损失曲线和精度曲线
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(range(num_epochs), train_losses, label='Train Loss')
plt.plot(range(num_epochs), test_losses, label='Test Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Training and Test Loss')

plt.subplot(1, 2, 2)
plt.plot(range(num_epochs), train_accuracies, label='Train Accuracy')
plt.plot(range(num_epochs), test_accuracies, label='Test Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Training and Test Accuracy')

plt.show()
